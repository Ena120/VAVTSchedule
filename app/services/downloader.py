import aiohttp
import asyncio
from bs4 import BeautifulSoup
import os
from urllib.parse import unquote

# –ë–∞–∑–æ–≤—ã–π URL
BASE_URL = "https://www.vavt.ru"
START_URL = "https://www.vavt.ru/schedule/?f=%D0%91%D0%B0%D0%BA%D0%B0%D0%BB%D0%B0%D0%B2%D1%80%D0%B8%D0%B0%D1%82&o=%D0%9E%D1%87%D0%BD%D0%B0%D1%8F+%D1%84%D0%BE%D1%80%D0%BC%D0%B0+%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F"
DOWNLOAD_DIR = "downloads"

# –ß—Ç–æ –º—ã –∏—â–µ–º
TARGET_FACULTIES = ['–ú–ü–§', '–§–í–ú', '–§–ú–§', '–§–≠–ú']
TARGET_COURSES = ['1 –∫—É—Ä—Å', '2 –∫—É—Ä—Å', '3 –∫—É—Ä—Å', '4 –∫—É—Ä—Å']

async def get_soup(session, url):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –¥–µ–ª–∞–µ—Ç –∏–∑ –Ω–µ–≥–æ —Å—É–ø"""
    try:
        async with session.get(url) as response:
            if response.status == 200:
                html = await response.text()
                return BeautifulSoup(html, "lxml")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}: {e}")
    return None

async def download_file(session, url, folder, filename):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é –ø–∞–ø–∫—É"""
    filepath = os.path.join(folder, filename)
    
    # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (—á—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –ª–∏—à–Ω–µ–µ)
    if os.path.exists(filepath):
        # print(f"‚è≠Ô∏è  –§–∞–π–ª —É–∂–µ –µ—Å—Ç—å: {filename}")
        return

    try:
        async with session.get(url) as response:
            if response.status == 200:
                content = await response.read()
                with open(filepath, 'wb') as f:
                    f.write(content)
                print(f"‚úÖ –°–∫–∞—á–∞–Ω: {filename}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {filename}: {e}")

async def main():
    connector = aiohttp.TCPConnector(ssl=False)
    async with aiohttp.ClientSession(connector=connector) as session:
        
        # 1. –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é, –∏—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç—ã
        print("üîç –®–∞–≥ 1: –ò—â–µ–º —Ñ–∞–∫—É–ª—å—Ç–µ—Ç—ã...")
        soup = await get_soup(session, START_URL)
        if not soup: return

        faculty_links = {}
        for a in soup.find_all('a', href=True):
            text = a.get_text(strip=True)
            if text in TARGET_FACULTIES:
                href = a['href']
                faculty_links[text] = href if href.startswith('http') else BASE_URL + href

        # 2. –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∞–∫—É–ª—å—Ç–µ—Ç—É
        for fac_name, fac_url in faculty_links.items():
            print(f"\nüèõÔ∏è  –§–∞–∫—É–ª—å—Ç–µ—Ç: {fac_name}")
            fac_soup = await get_soup(session, fac_url)
            if not fac_soup: continue

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫—É—Ä—Å—ã (1 –∫—É—Ä—Å, 2 –∫—É—Ä—Å...)
            course_links = {}
            for a in fac_soup.find_all('a', href=True):
                text = a.get_text(strip=True)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç "1 –∫—É—Ä—Å", "2 –∫—É—Ä—Å" –∏ —Ç.–¥.
                for course in TARGET_COURSES:
                    if course in text:
                        href = a['href']
                        course_links[course] = href if href.startswith('http') else BASE_URL + href
            
            # 3. –ó–∞—Ö–æ–¥–∏–º –Ω–∞ –∫–∞–∂–¥—ã–π –∫—É—Ä—Å –∏ –∫–∞—á–∞–µ–º PDF
            for course_name, course_url in course_links.items():
                print(f"  üéì {course_name}...")
                
                # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É: downloads/–ú–ü–§/1 –∫—É—Ä—Å
                course_dir = os.path.join(DOWNLOAD_DIR, fac_name, course_name)
                os.makedirs(course_dir, exist_ok=True)

                course_soup = await get_soup(session, course_url)
                if not course_soup: continue

                found_pdfs = 0
                for a in course_soup.find_all('a', href=True):
                    href = a['href']
                    
                    if '.pdf' in href.lower() and 'privacy-policy' not in href:
                        full_url = href if href.startswith('http') else BASE_URL + href
                        
                        # –î–æ—Å—Ç–∞–µ–º —á–∏—Å—Ç–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ —Å—Å—ã–ª–∫–∏ (–¥–µ–∫–æ–¥–∏—Ä—É–µ–º %20 –≤ –ø—Ä–æ–±–µ–ª—ã)
                        filename = unquote(href.split('/')[-1])
                        
                        await download_file(session, full_url, course_dir, filename)
                        found_pdfs += 1
                
                if found_pdfs == 0:
                    print(f"    ‚ö†Ô∏è PDF –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–≤–æ–∑–º–æ–∂–Ω–æ, —Ç–æ–ª—å–∫–æ —Å–µ—Å—Å–∏—è –∏–ª–∏ –∫–∞–Ω–∏–∫—É–ª—ã)")

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫
    asyncio.run(main())